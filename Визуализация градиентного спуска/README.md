Реализация различных градиентных методов оптимизации функции *n*-переменных. 

- Спуск с постоянным шагом;

- Спуск с переменным шагом по формуле:

  $$ \eta (t) = \frac{\eta ^0}{t + 1} $$, где $ t $ - номер текущей итерации (начиная с  $ t = 0 $);

- Ускоренный градиент Нестерова;

- AdaGrad;

- RMSProp.

Используется символьное вычисление градиента с помощью библиотеки *sympy*.

Процесс спуска для двух переменных визуализируется через линии уровня и в 3*D* (см. файл *grad_descent.html*)
